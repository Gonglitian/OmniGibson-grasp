{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.298 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from utils.data_utils import batch_inference_with_vlm\n",
    "from unsloth import FastVisionModel\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.Image.Image image mode=RGB size=640x480>},\n",
       "   {'type': 'text',\n",
       "    'text': '基于机器人的本体感知数据:[-0.60553426 -0.7151419   0.7958192   0.6989793   0.69278485 -0.08823238\\n  0.7962648   0.9906394   0.9695734  -0.7103372   0.9473687   0.7211443\\n  0.9960999  -0.6049482   0.1365048  -0.24480082  0.70386153  0.32014462\\n  0.5234661   0.21508121  0.7437057   0.06619466  0.20449932 -0.62042344\\n  0.75423676  0.03368961  0.04467025 -1.          0.2252341   0.35909453\\n  0.9739318   0.98581064  0.9659812  -0.46111378  0.99783456  0.9743047\\n  0.9333012  -0.2268411  -0.16786137  0.25861236  0.887341   -0.06577381\\n  0.45283127 -0.31233788  0.8008731   0.10091418  0.08861702  0.4467847\\n  0.8845037   0.01266375  0.00312535 -1.          0.31995556  0.02751317\\n -0.20706016 -0.17886077  0.2573864   0.9783282   0.9838744   0.9663086\\n -0.4117421   0.7287176   0.33601737  1.2706189   0.3597157   0.50089514]，确定下一步行动。'}]}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load pickle file\n",
    "with open(\"batch_data.pkl\", \"rb\") as f:\n",
    "    batch_data:dict = pickle.load(f) # type: dict\n",
    "\n",
    "messages = batch_data[\"messages\"]\n",
    "images = batch_data[\"rgb_images\"]\n",
    "messages = [ [m] for m in messages]\n",
    "images = [ [i] for i in images]\n",
    "messages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1416])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply chat template\n",
    "tokenizer.padding_side = \"left\"\n",
    "input_text = tokenizer.apply_chat_template(messages,add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    images,\n",
    "    input_text,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    ").to(\"cuda\")\n",
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "results = model.generate(**inputs,max_new_tokens = 128,use_cache=True,temperature=1.3,min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "基于机器人的本体感知数据:[ 2.7009381e-02  9.4238244e-02  9.9963516e-01  9.9554968e-01\n",
      " -4.5955130e-01 -1.7407253e-01  4.9118367e-01  1.0264358e-01\n",
      "  5.5968635e-02 -5.1123899e-01  4.0019637e-01  8.8815123e-01\n",
      "  9.8473281e-01  8.7105602e-01  9.9471819e-01  9.9843252e-01\n",
      "  8.5943860e-01  9.1642940e-01 -3.6645555e-01  1.0376803e+00\n",
      "  8.0084181e-01  5.4243791e-01  5.1614511e-01  4.5763934e-01\n",
      "  4.7950140e-01  1.2048919e-03  5.7669193e-03 -1.0000000e+00\n",
      "  4.2075670e-01  3.9120566e-02  3.3916235e-01  3.3890128e-02\n",
      " -4.9243736e-01 -1.3303520e-02 -4.6921518e-01  9.0717351e-01\n",
      "  9.9923450e-01  9.4072789e-01  9.9942559e-01  8.7034786e-01\n",
      "  9.9991149e-01  8.8308388e-01  3.4915221e-01 -1.0533147e+00\n",
      "  6.4651626e-01  8.4246987e-01 -5.2717876e-01  5.1323596e-02\n",
      " -9.8453596e-02  1.6180888e-02  7.2644786e-03 -1.0000000e+00\n",
      "  1.8311219e-04 -2.3660088e-04  1.3373031e-01  1.2152614e-01\n",
      "  5.1584029e-01  9.9101776e-01  9.9258822e-01 -8.5668474e-01\n",
      "  9.0751439e-02  1.8374002e-01 -2.3606814e-01  6.6481155e-01\n",
      "  8.5872464e-02  1.1384752e+01]，确定下一步行动。\n",
      "assistant\n",
      "根据提供的本体感知数据，机器人似乎在一个室内环境中，可能是房间或走廊的一部分。这些数据可能来自传感器，如激光雷达、摄像头或其他类型的传感器。\n",
      "\n",
      "为了确定下一步的行动，我们需要考虑以下几点：\n",
      "\n",
      "1. **环境理解**：首先，我们需要了解当前环境的具体情况。这包括墙壁的位置、门的位置、窗户的位置等。\n",
      "2. **目标定位**：我们需要明确我们的目标是什么。例如，我们是否需要找到一个特定的物体，进入某个房间，或者移动到某个位置。\n",
      "3. **路径规划**：一旦我们明确了目标和环境，我们可以开始规划路径。这通常\n",
      "****************************************************************************************************\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "基于机器人的本体感知数据:[-0.60553426 -0.7151419   0.7958192   0.6989793   0.69278485 -0.08823238\n",
      "  0.7962648   0.9906394   0.9695734  -0.7103372   0.9473687   0.7211443\n",
      "  0.9960999  -0.6049482   0.1365048  -0.24480082  0.70386153  0.32014462\n",
      "  0.5234661   0.21508121  0.7437057   0.06619466  0.20449932 -0.62042344\n",
      "  0.75423676  0.03368961  0.04467025 -1.          0.2252341   0.35909453\n",
      "  0.9739318   0.98581064  0.9659812  -0.46111378  0.99783456  0.9743047\n",
      "  0.9333012  -0.2268411  -0.16786137  0.25861236  0.887341   -0.06577381\n",
      "  0.45283127 -0.31233788  0.8008731   0.10091418  0.08861702  0.4467847\n",
      "  0.8845037   0.01266375  0.00312535 -1.          0.31995556  0.02751317\n",
      " -0.20706016 -0.17886077  0.2573864   0.9783282   0.9838744   0.9663086\n",
      " -0.4117421   0.7287176   0.33601737  1.2706189   0.3597157   0.50089514]，确定下一步行动。\n",
      "assistant\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(results, skip_special_tokens=True)\n",
    "print(len(outputs))\n",
    "for o in outputs:\n",
    "    print(o)\n",
    "    print(\"*\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-vla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
