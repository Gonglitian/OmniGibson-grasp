{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# VLM related\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from PIL import Image\n",
    "# OmniGibson related\n",
    "from envs.base_env import BaseEnvironment\n",
    "from omnigibson.utils.ui_utils import KeyboardRobotController\n",
    "from utils.debug import setup_debug_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 初始化Qwen VL模型\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_vlm(rgb_tensor, proprio_tensor, model, processor, process_vision_info, prompt_template=None):\n",
    "    \"\"\"\n",
    "    使用视觉语言模型对机器人观测进行推理\n",
    "    \n",
    "    参数:\n",
    "        rgb_tensor (torch.Tensor): RGB图像张量，形状为(H, W, C)\n",
    "        proprio_tensor (torch.Tensor): 机器人本体感知数据张量\n",
    "        model: VLM模型\n",
    "        processor: VLM处理器\n",
    "        process_vision_info: 处理视觉信息的函数\n",
    "        prompt_template (str, optional): 可选的提示模板，如果为None则使用默认模板\n",
    "        \n",
    "    返回:\n",
    "        str: 模型生成的文本结果\n",
    "    \"\"\"\n",
    "    # 处理RGB图像\n",
    "    if rgb_tensor.shape[2] == 4:\n",
    "        rgb_tensor = rgb_tensor[:, :, :3]\n",
    "    # 转换为PIL图像\n",
    "    rgb_img = Image.fromarray(rgb_tensor.numpy())\n",
    "    # 本体感知张量转换为numpy\n",
    "    proprio_vector = proprio_tensor.numpy()\n",
    "    \n",
    "    # 构建提示文本\n",
    "    prompt = f\"Based on the robot's proprioceptive data:{proprio_vector}, determine the next action.\"\n",
    "    if prompt_template:\n",
    "        prompt = prompt_template.format(proprio=proprio_vector)\n",
    "    \n",
    "    # 定义输入消息\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": rgb_img},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # 处理视觉信息\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # 准备模型输入\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 将输入移至适当的设备\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 执行推理\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # 解码生成的输出\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    # 返回生成的文本\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [omnigibson.simulator] ----- Starting \u001b[2m\u001b[1m\u001b[37mOmni\u001b[0m\u001b[1m\u001b[91mGibson\u001b[0m. This will take 10-30 seconds... -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在创建环境，这可能需要一些时间...\n",
      "[Warning] [omni.isaac.kit] Interactive python shell detected but ISAAC_JUPYTER_KERNEL was not set. Problems with asyncio may occur\n",
      "[Warning] [omni.isaac.kit] Please use Isaac Sim Python 3 kernel instead of the default Python 3 Kernel\n",
      "Starting kit application with the following args:  ['/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/isaacsim/exts/omni.isaac.kit/omni/isaac/kit/simulation_app.py', '/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/isaacsim/apps/omnigibson_4_1_0.kit', '--/app/tokens/exe-path=/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/omni', '--/persistent/app/viewport/displayOptions=3094', '--/rtx/materialDb/syncLoads=True', '--/rtx/hydra/materialSyncLoads=True', '--/omni.kit.plugin/syncUsdLoads=True', '--/app/renderer/resolution/width=1280', '--/app/renderer/resolution/height=720', '--/app/window/width=1440', '--/app/window/height=900', '--/renderer/multiGpu/enabled=False', '--/app/fastShutdown=True', '--ext-folder', '/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/isaacsim/exts', '--ext-folder', '/home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/isaacsim/apps', '--/physics/cudaDevice=0', '--portable', '--no-window', '--/app/window/hideUi=1']\n",
      "Passing the following args to the base kit application:  ['--f=/home/litiangong/.local/share/jupyter/runtime/kernel-v339b964dbdfe425c7d9032f21ea1c71e0c749be9b.json']\n",
      "[Info] [carb] Logging to file: /home/litiangong/miniconda3/envs/omnigibson/lib/python3.10/site-packages/omni/logs/Kit/Isaac-Sim/4.1/kit_20250427_030955.log\n",
      "2025-04-27 10:09:55 [0ms] [Warning] [omni.kit.app.plugin] No crash reporter present, dumps uploading isn't available.\n",
      "[0.054s] [ext: omni.kit.async_engine-0.0.0] startup\n",
      "[0.104s] [ext: omni.stats-1.0.1] startup\n",
      "[0.105s] [ext: omni.client-1.1.0] startup\n",
      "[0.119s] [ext: omni.datastore-0.0.0] startup\n",
      "[0.120s] [ext: omni.blobkey-1.1.0] startup\n",
      "[0.120s] [ext: omni.ujitso.default-1.0.0] startup\n",
      "[0.120s] [ext: omni.hsscclient-0.0.0] startup\n",
      "[0.121s] [ext: omni.rtx.shadercache.vulkan-1.0.0] startup\n",
      "[0.121s] [ext: omni.assets.plugins-0.0.0] startup\n",
      "[0.122s] [ext: omni.gpu_foundation-0.0.0] startup\n",
      "[0.127s] [ext: carb.windowing.plugins-1.0.0] startup\n",
      "2025-04-27 10:09:55 [119ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:55 [119ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "[0.128s] [ext: omni.kit.renderer.init-0.0.0] startup\n",
      "2025-04-27 10:09:55 [138ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.\n",
      "\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| Driver Version: 560.35.03     | Graphics API: Vulkan\n",
      "|=============================================================================================|\n",
      "| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |\n",
      "|     |                                  |        |     |            | Device-ID | UUID       |\n",
      "|     |                                  |        |     |            | Bus-ID    |            |\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| 0   | NVIDIA GeForce RTX 4090          | Yes: 0 |     | 24564   MB | 10de      | 0          |\n",
      "|     |                                  |        |     |            | 2684      | e0b97bd6.. |\n",
      "|     |                                  |        |     |            | 1         |            |\n",
      "|=============================================================================================|\n",
      "| OS: 22.04.4 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.4, Kernel: 6.5.0-18-generic\n",
      "| Processor: Intel(R) Core(TM) i9-14900KF | Cores: 24 | Logical: 48\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "| Total Memory (MB): 64007 | Free Memory: 48260\n",
      "| Total Page/Swap (MB): 2047 | Free Page/Swap: 1929\n",
      "|---------------------------------------------------------------------------------------------|\n",
      "[0.770s] [ext: omni.kit.pipapi-0.0.0] startup\n",
      "[0.771s] [ext: omni.kit.pip_archive-0.0.0] startup\n",
      "[0.771s] [ext: omni.mtlx-0.1.0] startup\n",
      "[0.772s] [ext: omni.usd.config-1.0.4] startup\n",
      "[0.773s] [ext: omni.gpucompute.plugins-0.0.0] startup\n",
      "[0.774s] [ext: omni.usd.libs-1.0.1] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [omni.kit.telemetry.impl.sentry_extension] sentry is disabled for external build\n",
      "[INFO] [omni.kit.telemetry.impl.sentry_extension] sentry is disabled for external build\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.858s] [ext: omni.kit.telemetry-0.5.0] startup\n",
      "[0.881s] [ext: omni.kit.loop-isaac-1.2.0] startup\n",
      "[0.883s] [ext: omni.kit.test-0.0.0] startup\n",
      "[0.905s] [ext: omni.appwindow-1.1.8] startup\n",
      "2025-04-27 10:09:55 [897ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:55 [897ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "[0.908s] [ext: omni.kit.renderer.core-1.0.1] startup\n",
      "2025-04-27 10:09:55 [901ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:55 [901ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "2025-04-27 10:09:55 [908ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:55 [908ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "[0.918s] [ext: omni.kit.renderer.capture-0.0.0] startup\n",
      "[0.919s] [ext: omni.kit.renderer.imgui-1.0.1] startup\n",
      "2025-04-27 10:09:56 [912ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:56 [912ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "2025-04-27 10:09:56 [913ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:56 [913ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "2025-04-27 10:09:56 [914ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:56 [914ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "[0.991s] [ext: omni.ui-2.23.11] startup\n",
      "[0.998s] [ext: omni.kit.mainwindow-1.0.3] startup\n",
      "[0.998s] [ext: carb.audio-0.1.0] startup\n",
      "[1.001s] [ext: omni.uiaudio-1.0.0] startup\n",
      "[1.002s] [ext: omni.kit.uiapp-0.0.0] startup\n",
      "[1.002s] [ext: omni.usd.schema.audio-0.0.0] startup\n",
      "[1.064s] [ext: omni.usd.schema.physx-106.0.20] startup\n",
      "[1.093s] [ext: omni.usd.schema.forcefield-106.0.20] startup\n",
      "[1.098s] [ext: omni.usd.schema.anim-0.0.0] startup\n",
      "[1.118s] [ext: omni.usd.schema.omniscripting-1.0.0] startup\n",
      "[1.123s] [ext: omni.usd.schema.omnigraph-1.0.0] startup\n",
      "[1.130s] [ext: omni.anim.graph.schema-106.0.2] startup\n",
      "[1.134s] [ext: omni.anim.navigation.schema-106.0.2] startup\n",
      "[1.138s] [ext: omni.usd.schema.isaac-2.1.0] startup\n",
      "[1.144s] [ext: omni.usd.schema.semantics-0.0.0] startup\n",
      "[1.147s] [ext: omni.usd.schema.geospatial-0.0.0] startup\n",
      "[1.149s] [ext: omni.usd.schema.scene.visualization-2.0.2] startup\n",
      "[1.150s] [ext: omni.graph.exec-0.9.3] startup\n",
      "[1.151s] [ext: omni.usd_resolver-1.0.0] startup\n",
      "[1.156s] [ext: omni.activity.core-1.0.1] startup\n",
      "[1.157s] [ext: omni.kit.usd_undo-0.1.8] startup\n",
      "[1.157s] [ext: omni.kit.exec.core-0.13.2] startup\n",
      "[1.158s] [ext: omni.usd.core-1.2.11] startup\n",
      "[1.164s] [ext: omni.kit.actions.core-1.0.0] startup\n",
      "[1.165s] [ext: omni.resourcemonitor-105.0.1] startup\n",
      "[1.166s] [ext: omni.kit.window.popup_dialog-2.0.24] startup\n",
      "[1.168s] [ext: omni.timeline-1.0.10] startup\n",
      "[1.169s] [ext: omni.kit.commands-1.4.9] startup\n",
      "[1.171s] [ext: usdrt.scenegraph-7.4.8] startup\n",
      "[1.205s] [ext: omni.kit.widget.nucleus_connector-1.1.8] startup\n",
      "[1.206s] [ext: omni.kit.audiodeviceenum-1.0.1] startup\n",
      "[1.206s] [ext: omni.hydra.usdrt_delegate-7.4.7] startup\n",
      "[1.237s] [ext: omni.hydra.scene_delegate-0.3.3] startup\n",
      "[1.248s] [ext: omni.kit.collaboration.telemetry-1.0.0] startup\n",
      "[1.249s] [ext: omni.usd-1.11.2] startup\n",
      "[1.292s] [ext: omni.kit.collaboration.channel_manager-1.0.11] startup\n",
      "[1.293s] [ext: omni.kit.usd.layers-2.1.31] startup\n",
      "[1.302s] [ext: omni.kit.collaboration.presence_layer-1.0.8] startup\n",
      "[1.303s] [ext: omni.kit.window.cursor-1.1.2] startup\n",
      "[1.304s] [ext: omni.kit.menu.utils-1.5.27] startup\n",
      "[1.311s] [ext: omni.iray.libs-0.0.0] startup\n",
      "[1.314s] [ext: omni.kit.primitive.mesh-1.0.16] startup\n",
      "[1.316s] [ext: omni.mdl.neuraylib-0.2.5] startup\n",
      "[1.317s] [ext: omni.hydra.engine.stats-1.0.2] startup\n",
      "[1.322s] [ext: omni.kit.widget.searchable_combobox-1.0.6] startup\n",
      "[1.323s] [ext: omni.kit.widget.path_field-2.0.9] startup\n",
      "[1.323s] [ext: omni.kit.clipboard-1.0.3] startup\n",
      "[1.324s] [ext: omni.ujitso.processor.texture-1.0.0] startup\n",
      "[1.324s] [ext: omni.kit.widget.browser_bar-2.0.10] startup\n",
      "[1.325s] [ext: omni.kit.notification_manager-1.0.8] startup\n",
      "[1.325s] [ext: omni.volume-0.5.0] startup\n",
      "[1.327s] [ext: omni.ujitso.client-0.0.0] startup\n",
      "[1.327s] [ext: omni.kit.helper.file_utils-0.1.8] startup\n",
      "[1.328s] [ext: omni.kit.widget.nucleus_info-1.0.2] startup\n",
      "[1.328s] [ext: omni.kit.widget.filebrowser-2.10.48] startup\n",
      "[1.330s] [ext: omni.kit.search_core-1.0.5] startup\n",
      "[1.330s] [ext: omni.kit.widget.options_menu-1.1.4] startup\n",
      "[1.332s] [ext: omni.ui.scene-1.9.3] startup\n",
      "[1.334s] [ext: omni.kit.widget.search_delegate-1.0.4] startup\n",
      "[1.335s] [ext: omni.kit.widget.options_button-1.0.2] startup\n",
      "[1.335s] [ext: omni.kit.widget.context_menu-1.2.1] startup\n",
      "[1.336s] [ext: omni.kit.hotkeys.core-1.3.3] startup\n",
      "[1.336s] [ext: omni.kit.window.filepicker-2.10.34] startup\n",
      "[1.340s] [ext: omni.hydra.rtx-0.2.0] startup\n",
      "[1.359s] [ext: omni.kit.context_menu-1.8.0] startup\n",
      "[1.360s] [ext: omni.mdl-52.0.1] startup\n",
      "[1.369s] [ext: omni.kit.window.file_importer-1.1.11] startup\n",
      "[1.369s] [ext: omni.kit.raycast.query-1.0.5] startup\n",
      "[1.381s] [ext: omni.kit.viewport.registry-104.0.6] startup\n",
      "[1.381s] [ext: omni.kit.viewport.legacy_gizmos-1.0.15] startup\n",
      "[1.387s] [ext: omni.kit.material.library-1.4.4] startup\n",
      "[1.389s] [ext: omni.kit.window.file_exporter-1.0.29] startup\n",
      "[1.389s] [ext: omni.kit.hydra_texture-1.2.6] startup\n",
      "[1.396s] [ext: omni.kit.window.drop_support-1.0.2] startup\n",
      "[1.396s] [ext: omni.kit.widget.viewport-106.0.3] startup\n",
      "[1.398s] [ext: omni.kit.widget.filter-1.1.4] startup\n",
      "[1.398s] [ext: omni.kit.viewport.window-106.0.8] startup\n",
      "[1.407s] [ext: omni.kit.widget.settings-1.1.1] startup\n",
      "[1.408s] [ext: omni.kit.widget.prompt-1.0.7] startup\n",
      "[1.408s] [ext: omni.kit.viewport.utility-1.0.17] startup\n",
      "[1.408s] [ext: omni.kit.window.preferences-1.5.3] startup\n",
      "[1.416s] [ext: omni.kit.widget.live_session_management.ui-1.0.1] startup\n",
      "[1.417s] [ext: omni.kit.viewport.actions-106.0.3] startup\n",
      "[1.418s] [ext: omni.kit.widget.stage-2.10.26] startup\n",
      "[1.424s] [ext: omni.kit.stage_template.core-1.1.21] startup\n",
      "[1.424s] [ext: omni.kit.widget.live_session_management-1.2.18] startup\n",
      "[1.425s] [ext: omni.kit.viewport.menubar.core-106.0.2] startup\n",
      "[1.430s] [ext: omni.kit.manipulator.transform-104.7.5] startup\n",
      "[1.431s] [ext: omni.kit.stage_templates-1.2.3] startup\n",
      "[1.432s] [ext: omni.debugdraw-0.1.3] startup\n",
      "[1.438s] [ext: omni.kit.viewport.menubar.display-106.0.2] startup\n",
      "[1.439s] [ext: omni.kvdb-106.0.20] startup\n",
      "[1.440s] [ext: omni.inspect-1.0.1] startup\n",
      "[1.441s] [ext: omni.kit.window.file-1.3.52] startup\n",
      "[1.442s] [ext: omni.kit.window.content_browser_registry-0.0.6] startup\n",
      "[1.443s] [ext: omni.usdphysics-106.0.20] startup\n",
      "[1.448s] [ext: omni.convexdecomposition-106.0.20] startup\n",
      "[1.454s] [ext: omni.kit.window.content_browser-2.9.14] startup\n",
      "[1.460s] [ext: omni.usdphysics.ui-106.0.20] startup\n",
      "[1.481s] [ext: omni.localcache-106.0.20] startup\n",
      "[1.481s] [ext: omni.physx.foundation-106.0.20] startup\n",
      "[1.482s] [ext: omni.kit.widget.highlight_label-1.0.2] startup\n",
      "[1.482s] [ext: omni.graph.core-2.170.3] startup\n",
      "[1.486s] [ext: omni.kit.widget.searchfield-1.1.6] startup\n",
      "[1.486s] [ext: omni.physx.cooking-106.0.20] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] [AutoNode] Defining data type 'any' as 'Any'\n",
      "[DEBUG] [AutoNode] Defining data type 'bool' as 'Bool' and array 'BoolArray\n",
      "[DEBUG] [AutoNode] Defining data type 'bundle' as 'Bundle'\n",
      "[DEBUG] [AutoNode] Defining data type 'colord[3]' as 'Color3d' and array 'Color3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'colorf[3]' as 'Color3f' and array 'Color3fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'colorh[3]' as 'Color3h' and array 'Color3hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'colord[4]' as 'Color4d' and array 'Color4dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'colorf[4]' as 'Color4f' and array 'Color4fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'colorh[4]' as 'Color4h' and array 'Color4hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'double' as 'Double' and array 'DoubleArray\n",
      "[DEBUG] [AutoNode] Defining data type 'double[2]' as 'Double2' and array 'Double2Array\n",
      "[DEBUG] [AutoNode] Defining data type 'double[3]' as 'Double3' and array 'Double3Array\n",
      "[DEBUG] [AutoNode] Defining data type 'double[4]' as 'Double4' and array 'Double4Array\n",
      "[DEBUG] [AutoNode] Defining data type 'execution' as 'Execution'\n",
      "[DEBUG] [AutoNode] Defining data type 'float' as 'Float' and array 'FloatArray\n",
      "[DEBUG] [AutoNode] Defining data type 'float[2]' as 'Float2' and array 'Float2Array\n",
      "[DEBUG] [AutoNode] Defining data type 'float[3]' as 'Float3' and array 'Float3Array\n",
      "[DEBUG] [AutoNode] Defining data type 'float[4]' as 'Float4' and array 'Float4Array\n",
      "[DEBUG] [AutoNode] Defining data type 'frame[4]' as 'Frame' and array 'FrameArray\n",
      "[DEBUG] [AutoNode] Defining data type 'half' as 'Half' and array 'HalfArray\n",
      "[DEBUG] [AutoNode] Defining data type 'half[2]' as 'Half2' and array 'Half2Array\n",
      "[DEBUG] [AutoNode] Defining data type 'half[3]' as 'Half3' and array 'Half3Array\n",
      "[DEBUG] [AutoNode] Defining data type 'half[4]' as 'Half4' and array 'Half4Array\n",
      "[DEBUG] [AutoNode] Defining data type 'int' as 'Int' and array 'IntArray\n",
      "[DEBUG] [AutoNode] Defining data type 'int[2]' as 'Int2' and array 'Int2Array\n",
      "[DEBUG] [AutoNode] Defining data type 'int[3]' as 'Int3' and array 'Int3Array\n",
      "[DEBUG] [AutoNode] Defining data type 'int[4]' as 'Int4' and array 'Int4Array\n",
      "[DEBUG] [AutoNode] Defining data type 'int64' as 'Int64' and array 'Int64Array\n",
      "[DEBUG] [AutoNode] Defining data type 'matrixd[2]' as 'Matrix2d' and array 'Matrix2dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'matrixd[3]' as 'Matrix3d' and array 'Matrix3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'matrixd[4]' as 'Matrix4d' and array 'Matrix4dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'normald[3]' as 'Normal3d' and array 'Normal3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'normalf[3]' as 'Normal3f' and array 'Normal3fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'normalh[3]' as 'Normal3h' and array 'Normal3hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'objectId' as 'ObjectId' and array 'ObjectIdArray\n",
      "[DEBUG] [AutoNode] Defining data type 'path' as 'Path'\n",
      "[DEBUG] [AutoNode] Defining data type 'pointd[3]' as 'Point3d' and array 'Point3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'pointf[3]' as 'Point3f' and array 'Point3fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'pointh[3]' as 'Point3h' and array 'Point3hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'quatd[4]' as 'Quatd' and array 'QuatdArray\n",
      "[DEBUG] [AutoNode] Defining data type 'quatf[4]' as 'Quatf' and array 'QuatfArray\n",
      "[DEBUG] [AutoNode] Defining data type 'quath[4]' as 'Quath' and array 'QuathArray\n",
      "[DEBUG] [AutoNode] Defining data type 'string' as 'String'\n",
      "[DEBUG] [AutoNode] Defining data type 'target' as 'Target'\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordd[2]' as 'TexCoord2d' and array 'TexCoord2dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordf[2]' as 'TexCoord2f' and array 'TexCoord2fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordh[2]' as 'TexCoord2h' and array 'TexCoord2hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordd[3]' as 'TexCoord3d' and array 'TexCoord3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordf[3]' as 'TexCoord3f' and array 'TexCoord3fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'texcoordh[3]' as 'TexCoord3h' and array 'TexCoord3hArray\n",
      "[DEBUG] [AutoNode] Defining data type 'timecode' as 'Timecode' and array 'TimecodeArray\n",
      "[DEBUG] [AutoNode] Defining data type 'token' as 'Token' and array 'TokenArray\n",
      "[DEBUG] [AutoNode] Defining data type 'uchar' as 'UChar' and array 'UCharArray\n",
      "[DEBUG] [AutoNode] Defining data type 'uint' as 'UInt' and array 'UIntArray\n",
      "[DEBUG] [AutoNode] Defining data type 'uint64' as 'UInt64' and array 'UInt64Array\n",
      "[DEBUG] [AutoNode] Defining data type 'vectord[3]' as 'Vector3d' and array 'Vector3dArray\n",
      "[DEBUG] [AutoNode] Defining data type 'vectorf[3]' as 'Vector3f' and array 'Vector3fArray\n",
      "[DEBUG] [AutoNode] Defining data type 'vectorh[3]' as 'Vector3h' and array 'Vector3hArray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.503s] [ext: omni.kit.widget.text_editor-1.0.2] startup\n",
      "[1.504s] [ext: omni.graph.image.core-0.3.2] startup\n",
      "[1.510s] [ext: omni.kit.window.property-1.11.1] startup\n",
      "[1.511s] [ext: omni.physx-106.0.20] startup\n",
      "[1.524s] [ext: omni.kit.widget.toolbar-1.6.2] startup\n",
      "[1.528s] [ext: omni.kit.property.usd-3.21.28] startup\n",
      "[1.532s] [ext: omni.physx.stageupdate-106.0.20] startup\n",
      "[1.533s] [ext: omni.physx.commands-106.0.20] startup\n",
      "[1.535s] [ext: omni.kit.manipulator.tool.snap-1.4.5] startup\n",
      "[1.537s] [ext: omni.graph.tools-1.78.0] startup\n",
      "[1.554s] [ext: omni.physx.ui-106.0.20] startup\n",
      "[1.576s] [ext: omni.graph-1.135.0] startup\n",
      "[1.597s] [ext: omni.physx.demos-106.0.20] startup\n",
      "[1.605s] [ext: omni.graph.image.nodes-1.0.2] startup\n",
      "[1.606s] [ext: omni.graph.action_core-1.1.4] startup\n",
      "[1.617s] [ext: omni.isaac.version-1.1.0] startup\n",
      "[1.618s] [ext: omni.syntheticdata-0.6.7] startup\n",
      "[1.630s] [ext: omni.physx.graph-106.0.20] startup\n",
      "[1.645s] [ext: omni.isaac.nucleus-0.3.0] startup\n",
      "[1.647s] [ext: omni.physx.telemetry-106.0.20] startup\n",
      "[1.652s] [ext: omni.kit.manipulator.selector-1.1.1] startup\n",
      "[1.656s] [ext: omni.ui_query-1.1.2] startup\n",
      "[1.657s] [ext: omni.kit.widget.graph-1.12.8] startup\n",
      "[1.663s] [ext: omni.kit.property.material-1.9.4] startup\n",
      "[1.665s] [ext: omni.kit.window.extensions-1.4.9] startup\n",
      "[1.669s] [ext: omni.kit.property.physx-106.0.20] startup\n",
      "[1.724s] [ext: omni.graph.ui-1.70.0] startup\n",
      "[1.731s] [ext: omni.physx.vehicle-106.0.20] startup\n",
      "[1.744s] [ext: omni.kit.manipulator.viewport-107.0.0] startup\n",
      "[1.746s] [ext: omni.kit.viewport.manipulator.transform-106.0.1] startup\n",
      "[1.747s] [ext: omni.kit.ui_test-1.2.18] startup\n",
      "[1.748s] [ext: omni.graph.action_nodes-1.23.0] startup\n",
      "[1.760s] [ext: omni.physx.camera-106.0.20] startup\n",
      "[1.769s] [ext: omni.fabric.commands-1.1.4] startup\n",
      "[1.779s] [ext: omni.kit.manipulator.prim.core-107.0.3] startup\n",
      "[1.784s] [ext: omni.physx.cct-106.0.20] startup\n",
      "[1.800s] [ext: omni.graph.action-1.102.1] startup\n",
      "[1.803s] [ext: omni.graph.nodes-1.143.0] startup\n",
      "[1.815s] [ext: omni.graph.ui_nodes-1.25.1] startup\n",
      "[1.823s] [ext: omni.kit.manipulator.prim.fabric-106.0.1] startup\n",
      "[1.825s] [ext: omni.physics.tensors-106.0.20] startup\n",
      "[1.834s] [ext: omni.kit.graph.delegate.default-1.2.2] startup\n",
      "[1.836s] [ext: omni.kit.manipulator.prim.usd-106.0.1] startup\n",
      "[1.837s] [ext: omni.graph.bundle.action-2.0.4] startup\n",
      "[1.837s] [ext: omni.command.usd-1.0.3] startup\n",
      "[1.838s] [ext: omni.physx.tensors-106.0.20] startup\n",
      "[1.846s] [ext: omni.kit.graph.editor.core-1.5.3] startup\n",
      "[1.850s] [ext: omni.kit.manipulator.prim-106.0.0] startup\n",
      "[1.851s] [ext: omni.kit.manipulator.selection-104.0.9] startup\n",
      "[1.853s] [ext: omni.kit.widget.layers-1.7.9] startup\n",
      "[1.875s] [ext: omni.kit.window.toolbar-1.6.1] startup\n",
      "[1.877s] [ext: omni.kit.graph.usd.commands-1.3.1] startup\n",
      "[1.878s] [ext: omni.kit.widget.material_preview-1.0.16] startup\n",
      "[1.879s] [ext: omni.physx.supportui-106.0.20] startup\n",
      "[1.897s] [ext: omni.warp.core-1.2.1] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Possible version incompatibility. Attempting to load omni::fabric::IPath with version v0.2 against v0.1.\n",
      "Warning: Possible version incompatibility. Attempting to load omni::fabric::IPath with version v0.2 against v0.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.943s] [ext: omni.kit.window.material_graph-1.8.15] startup\n",
      "[2.000s] [ext: omni.kit.numpy.common-0.1.2] startup\n",
      "[2.002s] [ext: omni.warp-1.2.1] startup\n",
      "[2.004s] [ext: omni.sensors.tiled-0.0.4] startup\n",
      "[2.011s] [ext: omni.physx.bundle-106.0.20] startup\n",
      "[2.011s] [ext: omni.graph.scriptnode-1.19.1] startup\n",
      "[2.012s] [ext: omni.isaac.dynamic_control-1.3.8] startup\n",
      "[2.028s] [ext: omni.replicator.core-1.11.14] startup\n",
      "2025-04-27 10:09:57 [2,045ms] [Warning] [omni.replicator.core.scripts.annotators] Annotator PostProcessDispatch is already registered, overwriting annotator template\n",
      "Warp 1.2.1 initialized:\n",
      "   CUDA Toolkit 11.8, Driver 12.6\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA GeForce RTX 4090\" (24 GiB, sm_89, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /home/litiangong/.cache/warp/1.2.1\n",
      "[2.091s] [ext: omni.isaac.core-3.18.1] startup\n",
      "[2.181s] [ext: omni.graph.visualization.nodes-2.1.1] startup\n",
      "[2.192s] [ext: omni.isaac.core_nodes-1.16.1] startup\n",
      "[2.208s] [ext: omni.isaac.cloner-0.8.1] startup\n",
      "[2.213s] [ext: omni.isaac.ui-0.16.0] startup\n",
      "[2.221s] [ext: omni.kit.graph.widget.variables-2.1.0] startup\n",
      "[2.225s] [ext: omni.kit.graph.delegate.modern-1.10.6] startup\n",
      "[2.231s] [ext: omni.isaac.wheeled_robots-2.3.3] startup\n",
      "[2.243s] [ext: omni.isaac.gym-0.11.3] startup\n",
      "[2.245s] [ext: omni.graph.window.core-1.109.0] startup\n",
      "[2.255s] [ext: omni.isaac.debug_draw-1.1.0] startup\n",
      "[2.263s] [ext: omni.isaac.block_world-1.0.0] startup\n",
      "[2.266s] [ext: omni.graph.window.generic-1.24.0] startup\n",
      "[2.273s] [ext: omni.isaac.menu-0.5.0] startup\n",
      "[2.277s] [ext: omni.isaac.occupancy_map-1.0.2] startup\n",
      "[2.286s] [ext: omni.importer.mjcf-1.1.1] startup\n",
      "[2.294s] [ext: omni.graph.window.action-1.26.0] startup\n",
      "[2.299s] [ext: omni.kit.widget.calendar-1.0.8] startup\n",
      "[2.301s] [ext: omni.kit.actions.window-1.1.1] startup\n",
      "[2.303s] [ext: omni.sensors.nv.common-1.2.2-isaac] startup\n",
      "[2.314s] [ext: omni.kit.widget.extended_searchfield-1.0.28] startup\n",
      "[2.318s] [ext: omni.kit.hotkeys.window-1.4.5] startup\n",
      "[2.322s] [ext: omni.kit.selection-0.1.4] startup\n",
      "[2.323s] [ext: omni.sensors.nv.materials-1.2.1-isaac] startup\n",
      "[2.324s] [ext: omni.isaac.kit-1.13.0] startup\n",
      "[2.325s] [ext: omni.kit.menu.create-1.0.13] startup\n",
      "[2.326s] [ext: omni.kit.menu.edit-1.1.24] startup\n",
      "[2.327s] [ext: omni.isaac.range_sensor-3.1.1] startup\n",
      "[2.337s] [ext: omni.sensors.nv.lidar-1.2.2-isaac] startup\n",
      "[2.344s] [ext: omni.sensors.nv.wpm-1.2.1-isaac] startup\n",
      "[2.345s] [ext: omni.kit.property.audio-1.0.11] startup\n",
      "[2.346s] [ext: omni.kit.widget.stage_icons-1.0.4] startup\n",
      "[2.347s] [ext: omni.kit.property.geometry-1.3.0] startup\n",
      "[2.349s] [ext: omni.kit.property.camera-1.0.6] startup\n",
      "[2.350s] [ext: omni.sensors.nv.radar-1.2.1-isaac] startup\n",
      "[2.357s] [ext: omni.hydra.scene_api-0.1.2] startup\n",
      "[2.369s] [ext: omni.kit.window.stage-2.5.10] startup\n",
      "[2.371s] [ext: omni.kit.property.render-1.1.1] startup\n",
      "[2.372s] [ext: omni.kit.property.light-1.0.8] startup\n",
      "[2.373s] [ext: omni.kit.menu.file-1.1.10] startup\n",
      "[2.374s] [ext: omni.kit.property.transform-1.5.1] startup\n",
      "[2.377s] [ext: omni.kit.menu.stage-1.2.5] startup\n",
      "[2.377s] [ext: omni.kit.profiler.window-2.2.1] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [omni.kit.profiler.window] remove _SpanInstance.__lt__ and use insort 'key' arg instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:09:57 [2,403ms] [Warning] [omni.kit.profiler.window] remove _SpanInstance.__lt__ and use insort 'key' arg instead\n",
      "[2.415s] [ext: omni.kit.property.bundle-1.2.11] startup\n",
      "[2.417s] [ext: omni.isaac.sensor-12.7.1] startup\n",
      "[2.457s] [ext: omni.kit.property.layer-1.1.6] startup\n",
      "[2.460s] [ext: omni.kit.stage_column.variant-1.0.13] startup\n",
      "[2.463s] [ext: omni.kit.stage_column.payload-2.0.0] startup\n",
      "[2.465s] [ext: omni.isaac.scene_blox-0.1.2] startup\n",
      "[2.467s] [ext: omni.isaac.quadruped-1.4.5] startup\n",
      "[2.477s] [ext: omni.kit.viewport.menubar.camera-105.1.8] startup\n",
      "[2.484s] [ext: omni.isaac.lula-3.0.1] startup\n",
      "[2.488s] [ext: omni.isaac.surface_gripper-1.0.1] startup\n",
      "[2.491s] [ext: omni.kit.viewport.menubar.settings-106.0.1] startup\n",
      "[2.495s] [ext: omni.kit.viewport.menubar.render-106.1.3] startup\n",
      "[2.497s] [ext: omni.kit.manipulator.camera-105.0.5] startup\n",
      "[2.503s] [ext: omni.isaac.motion_generation-7.1.0] startup\n",
      "[2.505s] [ext: omni.isaac.manipulators-2.1.0] startup\n",
      "[2.507s] [ext: omni.kit.viewport.bundle-104.0.1] startup\n",
      "[2.509s] [ext: omni.kit.viewport.menubar.lighting-106.0.2] startup\n",
      "[2.510s] [ext: omni.isaac.universal_robots-0.3.5] startup\n",
      "[2.511s] [ext: omni.kit.ui.actions-1.0.1] startup\n",
      "[2.514s] [ext: omni.kit.widget.timeline-105.0.1] startup\n",
      "[2.519s] [ext: omni.kit.window.commands-0.2.5] startup\n",
      "[2.520s] [ext: omni.kit.window.console-0.2.12] startup\n",
      "[2.535s] [ext: omni.kit.window.script_editor-1.7.6] startup\n",
      "[2.536s] [ext: omni.kit.menu.common-1.1.5] startup\n",
      "[2.537s] [ext: omni.kit.window.status_bar-0.1.6] startup\n",
      "[2.543s] [ext: omni.rtx.window.settings-0.6.16] startup\n",
      "[2.545s] [ext: omni.kit.window.title-1.1.3] startup\n",
      "2025-04-27 10:09:57 [2,538ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:57 [2,538ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "2025-04-27 10:09:57 [2,539ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.\n",
      "2025-04-27 10:09:57 [2,539ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "[2.548s] [ext: omni.isaac.franka-0.4.1] startup\n",
      "[2.549s] [ext: omni.replicator.isaac-1.15.0] startup\n",
      "[2.557s] [ext: omni.replicator.replicator_yaml-2.0.5] startup\n",
      "[2.563s] [ext: omni.rtx.settings.core-0.6.0] startup\n",
      "[2.569s] [ext: omni.kit.widget.live-2.1.6] startup\n",
      "[2.572s] [ext: omni.isaac.cortex-0.3.8] startup\n",
      "[2.573s] [ext: omni.kit.viewport.rtx-104.0.1] startup\n",
      "[2.575s] [ext: semantics.schema.editor-0.3.6] startup\n",
      "[2.576s] [ext: semantics.schema.property-1.0.3] startup\n",
      "[2.578s] [ext: omni.kit.widget.cache_indicator-2.0.8] startup\n",
      "[2.579s] [ext: omni.isaac.utils-1.0.1] startup\n",
      "2025-04-27 10:09:57 [2,571ms] [Warning] [omni.kit.widget.cache_indicator.cache_state_menu] Unable to detect Omniverse Cache Server. File /home/litiangong/.nvidia-omniverse/config/omniverse.toml is not found. Consider installing it for better IO performance.\n",
      "[2.583s] [ext: omni.isaac.cortex.sample_behaviors-1.0.5] startup\n",
      "[2.584s] [ext: omni.importer.urdf-1.14.1] startup\n",
      "[2.614s] [ext: omni.kit.window.stats-0.1.6] startup\n",
      "[2.618s] [ext: omnigibson_4_1_0-4.1.0] startup\n",
      "[2.619s] Simulation App Starting\n",
      "2025-04-27 10:09:57 [2,649ms] [Warning] [omni.kvdb.plugin] Disabling key-value database because another kit process is locking it\n",
      "[3.915s] app ready\n",
      "2025-04-27 10:09:59 [3,972ms] [Warning] [omni.kit.imgui_renderer.plugin] _createExtendCursor: No windowing.\n",
      "2025-04-27 10:09:59 [3,972ms] [Warning] [omni.kit.imgui_renderer.plugin] _createExtendCursor: No windowing.\n",
      "[4.491s] Simulation App Startup Complete\n",
      "[4.581s] [ext: omni.usd.schema.flow-106.0.8] startup\n",
      "[4.582s] [ext: omni.flowusd-106.0.15] startup\n",
      "2025-04-27 10:09:59 [4,580ms] [Warning] [omni.stageupdate.plugin] Deprecated: direct use of IStageUpdate callbacks is deprecated. Use IStageUpdate::getStageUpdate instead.\n",
      "[4.596s] [ext: omni.kit.widget.zoombar-1.0.5] startup\n",
      "[4.599s] [ext: omni.scene.visualization.core-105.4.13] startup\n",
      "2025-04-27 10:09:59 [4,596ms] [Warning] [omni.stageupdate.plugin] Deprecated: direct use of IStageUpdate callbacks is deprecated. Use IStageUpdate::getStageUpdate instead.\n",
      "[4.613s] [ext: omni.ramp-105.1.15] startup\n",
      "[4.621s] [ext: omni.kit.browser.core-2.3.11] startup\n",
      "[4.623s] [ext: omni.particle.system.core-105.1.6] startup\n",
      "2025-04-27 10:09:59 [4,622ms] [Warning] [omni.particle.system.core.scripts.extension] \n",
      "            ATTENTION!: omni.particle.system.core is currently undergoing extensive (breaking) changes.\n",
      "            Please be aware that systems built with the existing extension will need to be rebuilt in\n",
      "            the USD Composer 2023.2 release.\n",
      "[4.637s] [ext: omni.kit.browser.folder.core-1.9.12] startup\n",
      "[4.639s] [ext: omni.particle.system.ui-105.1.10] startup\n",
      "[4.723s] [ext: omni.graph.window.particle.system-105.1.18] startup\n",
      "DYNAMICS_TODO:\n",
      "  Make sure \"omni.particle.system.core2\" is a proper dependency.\n",
      "  For now, it can just be set to autoload.\n",
      "DYNAMICS_TODO:\n",
      "  Make sure \"omni.particle.system.core2\" is a proper dependency.\n",
      "  For now, it can just be set to autoload.\n",
      "[4.750s] [ext: omni.particle.system.bundle-105.1.0] startup\n",
      "[4.756s] [ext: omni.anim.behavior.schema-106.0.1] startup\n",
      "[4.762s] [ext: omni.kit.widget.imageview-1.0.3] startup\n",
      "[4.763s] [ext: omni.kit.window.quicksearch-2.4.4] startup\n",
      "[4.765s] [ext: omni.genproc.core-105.1.9] startup\n",
      "[4.777s] [ext: omni.curve.creator-105.0.4] startup\n",
      "[4.779s] [ext: omni.anim.asset-106.0.3] startup\n",
      "[4.785s] [ext: omni.anim.retarget.core-106.0.1] startup\n",
      "[4.792s] [ext: omni.anim.graph.core-106.0.6] startup\n",
      "2025-04-27 10:09:59 [4,796ms] [Warning] [omni.graph.core._impl._registration.register_python_ogn] Python import process in omni.anim.graph.core failed - 'NoneType' object is not subscriptable. Aborting Python node registration\n",
      "[4.805s] [ext: omni.curve.manipulator-105.2.6] startup\n",
      "[4.843s] [ext: omni.graph.io-1.9.1] startup\n",
      "2025-04-27 10:09:59 [4,834ms] [Warning] [omni.graph.io._impl.extension] omni.graph.io is deprecated. Nodes in this extension have been moved to omni.graph.nodes. Update any dependencies accordingly.\n",
      "[4.844s] [ext: omni.kit.browser.sample-1.4.7] startup\n",
      "[4.869s] [ext: omni.anim.graph.ui-106.0.1] startup\n",
      "[4.923s] [ext: omni.anim.graph.bundle-106.0.3] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Possible version incompatibility. Attempting to load omni::fabric::IPath with version v0.2 against v0.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.924s] [ext: omni.kit.quicklayout-1.0.7] startup\n",
      "[4.949s] [ext: omni.anim.navigation.core-106.0.2] startup\n",
      "[4.957s] [ext: omni.anim.navigation.ui-106.0.2] startup\n",
      "[5.009s] [ext: omni.anim.navigation.bundle-106.0.1] startup\n",
      "[5.010s] [ext: omni.anim.skelJoint-106.0.1] startup\n",
      "[5.019s] [ext: omni.anim.retarget.ui-106.0.1] startup\n",
      "[5.048s] [ext: omni.anim.retarget.bundle-106.0.1] startup\n",
      "[5.049s] [ext: omni.kit.scripting-106.0.1] startup\n",
      "[5.052s] [ext: omni.anim.people-0.4.1] startup\n",
      "[5.067s] [ext: omni.anim.curve.core-1.1.13] startup\n",
      "[5.084s] [ext: omni.anim.timeline-105.0.23] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Possible version incompatibility. Attempting to load omni::fabric::IPath with version v0.2 against v0.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.131s] [ext: omni.kit.streamsdk.plugins-3.2.1] startup\n",
      "[5.136s] [ext: omni.kit.renderer.cuda_interop-1.0.1] startup\n",
      "[5.137s] [ext: omni.kit.livestream.core-3.2.0] startup\n",
      "[5.142s] [ext: omni.kit.livestream.native-4.1.0] startup\n",
      "\n",
      "Active user not found. Using default user [kiosk]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:10:05 [10,597ms] [Error] [carb.livestream.plugin] Stream Server: starting the server failed, 0x800B1002\n",
      "2025-04-27 10:10:05 [10,597ms] [Error] [carb.livestream.plugin] Could not initialize streaming components\n",
      "2025-04-27 10:10:05 [10,597ms] [Error] [carb.livestream.plugin] Couldn't initialize the capture device.\n",
      "Warning: Possible version incompatibility. Attempting to load omni::fabric::IStageReaderWriter with version v0.10 against v0.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now streaming on 169.235.18.46 via Omniverse Streaming Client\n",
      "[10.843s] [ext: omni.physx.fabric-106.0.20] startup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [omnigibson.simulator] ---------- Welcome to \u001b[2m\u001b[1m\u001b[37mOmni\u001b[0m\u001b[1m\u001b[91mGibson\u001b[0m! ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:10:06 [11,061ms] [Warning] [omni.syntheticdata.plugin] OgnSdPostRenderVarToHost : rendervar copy from texture directly to host buffer is counter-performant. Please use copy from texture to device buffer first.\n",
      "2025-04-27 10:10:06 [11,092ms] [Warning] [omni.fabric.plugin] removePath called on non-existent path /Render/RenderProduct_Replicator/PostRender/SDGPipeline/RenderProduct_Replicator_GpuInteropEntry \n",
      "\n",
      "\n",
      "\u001b[1m\u001b[37m                   ___________\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m_\u001b[0m\n",
      "\u001b[1m\u001b[37m                  /          \u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m/ \\\u001b[0m\n",
      "\u001b[1m\u001b[37m                 /          \u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m/ /\u001b[0m\u001b[1m\u001b[37m__\u001b[0m\u001b[1m\u001b[91m\u001b[0m\n",
      "\u001b[1m\u001b[37m                /          \u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m/ /  /\\\u001b[0m\n",
      "\u001b[1m\u001b[37m               /\u001b[0m\u001b[2m\u001b[1m\u001b[37m__________\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m/ /\u001b[0m\u001b[1m\u001b[37m__\u001b[0m\u001b[1m\u001b[91m/  \\\u001b[0m\n",
      "\u001b[1m\u001b[37m               \u001b[0m\u001b[2m\u001b[1m\u001b[37m\\   _____  \u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\\ \\\u001b[0m\u001b[1m\u001b[37m__\u001b[0m\u001b[1m\u001b[91m\\  /\u001b[0m\n",
      "\u001b[1m\u001b[37m                \u001b[0m\u001b[2m\u001b[1m\u001b[37m\\  \\  \u001b[0m\u001b[1m\u001b[37m/ \u001b[0m\u001b[2m\u001b[1m\u001b[37m\\  \u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\\ \\_/ /\u001b[0m\n",
      "\u001b[1m\u001b[37m                 \u001b[0m\u001b[2m\u001b[1m\u001b[37m\\  \\\u001b[0m\u001b[1m\u001b[37m/\u001b[0m\u001b[2m\u001b[1m\u001b[37m___\\  \u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\\   /\u001b[0m\n",
      "\u001b[1m\u001b[37m                  \u001b[0m\u001b[2m\u001b[1m\u001b[37m\\__________\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[2m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\u001b[0m\u001b[1m\u001b[37m\u001b[0m\u001b[1m\u001b[91m\\_/  \u001b[0m\n",
      "\u001b[2m\u001b[1m\u001b[37m       ___                  _\u001b[0m\u001b[1m\u001b[91m  ____ _ _                     \u001b[0m\n",
      "\u001b[2m\u001b[1m\u001b[37m      / _ \\ _ __ ___  _ __ (_)\u001b[0m\u001b[1m\u001b[91m/ ___(_) |__  ___  ___  _ __  \u001b[0m\n",
      "\u001b[2m\u001b[1m\u001b[37m     | | | | '_ ` _ \\| '_ \\| |\u001b[0m\u001b[1m\u001b[91m |  _| | '_ \\/ __|/ _ \\| '_ \\ \u001b[0m\n",
      "\u001b[2m\u001b[1m\u001b[37m     | |_| | | | | | | | | | |\u001b[0m\u001b[1m\u001b[91m |_| | | |_) \\__ \\ (_) | | | |\u001b[0m\n",
      "\u001b[2m\u001b[1m\u001b[37m      \\___/|_| |_| |_|_| |_|_|\u001b[0m\u001b[1m\u001b[91m\\____|_|_.__/|___/\\___/|_| |_|\u001b[0m\n",
      "\n",
      "2025-04-27 10:10:07 [12,883ms] [Warning] [omni.syntheticdata.plugin] SdRenderVarPtr missing valid input renderVar LdrColorSDhost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [omnigibson.simulator] Imported scene 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:10:08 [13,099ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "期望的物体总数: 7\n",
      "分配给各类别的数量: [1, 1, 1, 1, 1, 1, 1]\n",
      "桌子包围盒中心: tensor([ 1.4889, -0.5252,  0.3084])\n",
      "桌子包围盒尺寸: tensor([1.1380, 0.7617, 0.6504])\n",
      "桌子高度: 0.6335694789886475\n",
      "桌子朝向: tensor([0.0000, 0.0000, 0.7071, 0.7071])\n",
      "total_grid_cells: 28\n",
      "桌面尺寸: 1.0779612064361572 x 0.7016814351081848\n",
      "网格大小: 0.15 x 0.15\n",
      "网格数量: 7 x 4 = 28\n",
      "占用率: 0.8\n",
      "可用位置数量: 22\n",
      "生成了 22 个位置点\n",
      "根据网格和占用率生成的可用位置数量: 22\n",
      "可用位置(22)多于期望物体总数(7)，可以补充更多物体\n",
      "最终的物体分配数量: [4, 3, 3, 3, 3, 3, 3]\n",
      "最终的物体总数: 22\n",
      "已生成 22 个物品配置，使用位置数量: 22\n",
      "2025-04-27 10:10:10 [15,898ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "成功添加了 22 个动态物体\n",
      "等待场景稳定...\n",
      "场景稳定完成\n",
      "已设置机器人初始关节位置\n",
      "已将当前关节位置设为默认重置位置\n",
      "环境创建完成！\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，机器人似乎处于一个混乱的环境中，桌面上散落着各种物品和工具。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌面上有多个碗、盘子、杯子、刀具、勺子等餐具，还有一些可能是厨房用具的物品。\n",
      "2. **工具位置**：有几个工具（如铲子、夹子）放在桌面上，但它们的位置并不明确。\n",
      "3. **机器人状态**：机器人似乎在尝试处理这些物品，但由于其状态数据中包含大量负\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在一个厨房环境中，桌面上有各种物品，包括锅、碗、勺子、刀具等。根据这些信息，我们可以推断出以下几点：\n",
      "\n",
      "1. **物体位置和状态**：\n",
      "   - 桌面上有多个锅和碗。\n",
      "   - 有一些勺子和刀具散落在桌子上。\n",
      "   - 有一个杯子在桌子的一侧。\n",
      "\n",
      "2. **动作历史**：\n",
      "   - 状态数据中包含了一些动作指令，如移动、旋转等，但具体动作内容没有详细描述。\n",
      "\n",
      "3. **当前任务目标**：\n",
      "   -\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，这似乎是一个机器人在执行任务时的状态信息。以下是对当前场景和下一步操作的分析：\n",
      "\n",
      "### 当前场景分析：\n",
      "1. **物体位置**：桌子上有多个物品，包括一个杯子、一个碗、一个盘子、一个勺子、一个刀、一个橙色的物体（可能是水果）、一个黑色的物体（可能是锅或容器）等。\n",
      "2. **机器人状态**：机器人似乎处于一个特定的位置，可能是在准备进行某种操作。\n",
      "\n",
      "### 下一步操作建议：\n",
      "1. **识别目标**：首先需要确定机器人要完成的任务是什么。\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在一个厨房环境中，桌面上有各种物品，包括锅、碗、勺子和一些食物。根据这些信息，我可以给出以下建议：\n",
      "\n",
      "1. **清洁桌面**：首先，机器人需要清理桌面上的食物残渣和其他杂物，以保持卫生。\n",
      "\n",
      "2. **准备食材**：如果桌子上有未处理的食物，机器人可以开始准备食材。例如，如果有一个苹果，机器人可以拿起它并将其放入一个干净的碗中。\n",
      "\n",
      "3. **烹饪准备**：如果桌子上有一个锅，机器人可以将锅放在炉灶上，并准备开始烹饪。\n",
      "\n",
      "4\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在处理一个厨房环境中的多个物品。以下是对当前场景的分析和建议的操作步骤：\n",
      "\n",
      "### 当前场景分析：\n",
      "1. **物品分布**：桌面上有多个物品，包括碗、盘子、杯子、调料瓶等。\n",
      "2. **机器人位置**：机器人位于桌子的两侧，可能正在准备进行某种操作。\n",
      "\n",
      "### 建议的操作步骤：\n",
      "1. **识别目标物品**：\n",
      "   - 确定需要处理的目标物品（例如，可能是其中一个碗或盘子）。\n",
      "   - 使用视觉传感器来识别这些物品的位置和\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在一个厨房环境中，周围有各种物品和工具。以下是对当前场景的分析：\n",
      "\n",
      "1. **物品分布**：\n",
      "   - 左上角有一个杯子。\n",
      "   - 中间有一些食物（如橙色的物体）和调料容器。\n",
      "   - 右侧有一个绿色的水果（可能是苹果）。\n",
      "   - 中间有一个黑色的锅。\n",
      "   - 右下角有一个白色的碗。\n",
      "\n",
      "2. **工具和设备**：\n",
      "   - 左侧有一个白色的电器（可能是咖啡机或搅拌机）。\n",
      "   - 右侧有一个白色的电器（可能是\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，机器人似乎处于一个混乱的环境中，桌面上散落着各种物品和工具。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌面上有多个碗、盘子、杯子、刀具等厨房用具。\n",
      "2. **工具位置**：有几个工具（如铲子、勺子）放在不同的位置。\n",
      "3. **机器人状态**：机器人似乎在尝试处理这些物品或工具。\n",
      "\n",
      "### 可能的操作步骤：\n",
      "1. **清理桌面**：\n",
      "   - 使用铲子或勺\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种物品和可能的动作。以下是对当前场景的分析以及建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：\n",
      "   - 桌子上散落着多个物品，包括碗、盘子、杯子、调料瓶等。\n",
      "   - 桌子上有食物残渣，表明之前有人使用过。\n",
      "\n",
      "2. **机器人位置**：\n",
      "   - 机器人位于桌子下方，可能正在准备进行某种动作或清理工作。\n",
      "\n",
      "3. **机器人状态**：\n",
      "   - 状态数据中包含了\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含了各种物品和可能的动作指令。以下是对当前场景的分析以及一些建议：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：\n",
      "   - 有多个杯子、碗、盘子等餐具。\n",
      "   - 一些食物（如橙色的物体）散落在桌子上。\n",
      "   - 一个黑色的锅放在桌子中央。\n",
      "\n",
      "2. **动作指令**：\n",
      "   - 状态数据中包含了一些数值，可能是动作指令或传感器读数。例如，`[ 1.39876\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种物品和工具。以下是对当前场景的分析：\n",
      "\n",
      "### 环境描述：\n",
      "1. **桌子**：上面有多个物品，包括碗、杯子、调料瓶、刀具等。\n",
      "2. **椅子**：在桌子前面有一把红色的椅子。\n",
      "\n",
      "### 物品分布：\n",
      "- **碗**：有几个大小不一的碗，其中一个较大且灰色。\n",
      "- **杯子**：有几个白色的杯子。\n",
      "- **调料瓶**：一个红色的调料瓶。\n",
      "- **刀具**：一把刀放在\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含了各种物品和可能的动作指令。以下是对当前场景的分析以及一些建议：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌面上有多个物品，包括但不限于碗、杯子、盘子、刀具等。\n",
      "2. **动作指令**：状态数据中包含了一些数值，这些数值可能是机器人需要执行的动作或目标位置。\n",
      "\n",
      "### 建议的操作步骤：\n",
      "1. **识别目标物体**：\n",
      "   - 首先，你需要确定哪些物体是你的目标。例如，如果你\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，机器人似乎处于一个厨房环境中，桌面上有各种物品，包括碗、杯子、刀具等。根据这些信息，我们可以推测以下几点：\n",
      "\n",
      "1. **物体识别**：机器人可能需要识别桌子上的各个物体，以便进行相应的操作。\n",
      "2. **动作规划**：机器人需要规划如何移动和操作这些物体。\n",
      "\n",
      "### 分析场景\n",
      "\n",
      "1. **桌面布局**：\n",
      "   - 左上角有一个白色的杯子。\n",
      "   - 中间有一个灰色的碗。\n",
      "   - 右上角有一个绿色的水果（可能是苹果）。\n",
      "   - 中间有一些\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种厨房用具和物品。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：场景中散布着多个碗、盘子、锅、刀、勺子等厨房用具。\n",
      "2. **物体状态**：有些物体是空的（如碗），而有些物体可能是脏的或有残留物（如盘子）。\n",
      "3. **机器人位置**：机器人位于桌子下方，可能正在准备进行某种操作。\n",
      "\n",
      "### 下一步操作建议：\n",
      "\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在一个厨房环境中，桌面上有各种物品，包括碗、杯子、锅和刀具等。根据这些信息，我可以提供以下建议：\n",
      "\n",
      "1. **清洁桌面**：首先，机器人应该清理桌面上的所有物品，以确保工作区域整洁。这可能包括将碗、杯子和其他物品移到一边。\n",
      "\n",
      "2. **检查物品位置**：在清理桌面后，机器人需要检查每个物品的位置，以便确定它们是否需要移动或放置到其他地方。\n",
      "\n",
      "3. **准备烹饪工具**：如果机器人正在准备烹饪，它应该检查锅和刀具的位置\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种厨房用具和物品。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌子上散落着多个物品，包括碗、杯子、刀具、锅等。\n",
      "2. **机器人位置**：机器人位于桌子的一侧，看起来像是在准备进行某种动作。\n",
      "\n",
      "### 下一步操作建议：\n",
      "1. **识别目标物体**：首先，机器人需要识别出它想要操作的目标物体。例如，如果它想要拿起一个碗或刀具，它\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境中的机器人任务。根据当前的状态数据，我们可以看到一些物体的位置和状态信息。以下是对场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：\n",
      "   - 桌子上有多个物品，包括碗、杯子、刀具等。\n",
      "   - 物体分布在桌子的不同位置。\n",
      "\n",
      "2. **物体状态**：\n",
      "   - 大部分物体处于静止状态，没有明显的移动或动作。\n",
      "   - 可能有一些物体正在被机器人识别或处理。\n",
      "\n",
      "### 下一步操作\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，这似乎是一个机器人在处理各种物品的场景。以下是对当前场景的分析和建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌面上有多个物品，包括碗、杯子、刀具等。\n",
      "2. **机器人位置**：机器人位于桌子的左侧，可能正在准备进行某种操作。\n",
      "\n",
      "### 状态数据解读：\n",
      "- **动作历史**：从状态数据中可以看出，机器人已经执行了一些动作，但具体动作内容不明确。\n",
      "- **目标位置**：根据状态数据中的坐标信息，机器人可能需要移动到某个特定的位置\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在处理一个厨房环境中的多个物体。以下是对当前场景的分析和建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **桌子上的物品**：\n",
      "   - 多个碗、盘子、杯子等。\n",
      "   - 削皮器、刀具、勺子等厨房用具。\n",
      "   - 某些物品可能被移动或放置不当。\n",
      "\n",
      "2. **机器人状态**：\n",
      "   - 状态数据中包含许多负值，这表明机器人可能处于一种混乱或错误的状态。\n",
      "   - 特别是接近0的位置（如第\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在处理一个复杂的任务，可能是在进行某种清洁或维护工作。以下是对当前场景的分析和建议的操作步骤：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：桌子上有多个物品，包括碗、杯子、盘子、刀具等。\n",
      "2. **机器人状态**：机器人处于运动状态，正在移动和操作。\n",
      "\n",
      "### 建议操作步骤：\n",
      "1. **确认目标**：首先，确保机器人知道它需要完成的任务是什么。这可以通过检查其目标坐标或任务指令来实现。\n",
      "2. **清理桌面**：如果\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，这似乎是一个机器人在厨房环境中执行任务的状态。以下是对当前场景的分析和建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：桌子上有多个物品，包括碗、杯子、刀具、锅等。\n",
      "2. **动作历史**：机器人可能正在处理或准备食物。\n",
      "\n",
      "### 建议的操作：\n",
      "1. **清理桌面**：\n",
      "   - 使用吸尘器（假设存在）清理桌面上的食物残渣和其他杂物。\n",
      "   - 将垃圾放入垃圾桶中。\n",
      "\n",
      "2. **整理工具**：\n",
      "   - 将刀具、\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎在一个厨房环境中，周围有各种厨房用具和物品。以下是对当前场景的分析：\n",
      "\n",
      "1. **物体位置**：\n",
      "   - 左上角有一个杯子。\n",
      "   - 中间有一个灰色的碗。\n",
      "   - 右上角有一个绿色的水果（可能是苹果）。\n",
      "   - 中间右侧有一个白色的碗。\n",
      "   - 中间左侧有一个黑色的锅。\n",
      "   - 中间右侧还有一个黑色的锅。\n",
      "   - 中间左侧有一个红色的物体（可能是调料瓶或罐子）。\n",
      "\n",
      "2. **动作历史**：\n",
      "   - 最近的动作\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种烹饪工具和食材。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **工具和物品**：\n",
      "   - 咖啡机（位于左侧）\n",
      "   - 砧板（位于中心）\n",
      "   - 刀具（多个刀具散落在周围）\n",
      "   - 盘子（一个盘子在砧板上）\n",
      "   - 水壶（位于右侧）\n",
      "\n",
      "2. **食材**：\n",
      "   - 柠檬（位于右上角）\n",
      "   -\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含了各种物体和可能的动作指令。以下是对当前场景的分析以及一些建议：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：场景中包含多个物体，如碗、杯子、盘子等。\n",
      "2. **动作指令**：状态数据中包含了一系列数字，这些数字可能是动作指令或传感器读数。\n",
      "\n",
      "### 建议的操作步骤：\n",
      "1. **识别目标物体**：\n",
      "   - 确定当前需要处理的目标物体。例如，如果目标是“拿起一个碗”，则\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个厨房环境中的场景。以下是对当前场景的分析和建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：\n",
      "   - 碗、盘子、勺子、叉子等餐具散落在桌子上。\n",
      "   - 水果（如苹果、橙子）和蔬菜（如胡萝卜）也分布在桌子上。\n",
      "   - 咖啡机、咖啡杯、调料瓶等厨房用具也在桌子上。\n",
      "\n",
      "2. **机器人位置**：\n",
      "   - 机器人位于桌子下方，可能正在准备或清理桌面。\n",
      "\n",
      "### 建\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境，其中包含各种烹饪工具和食材。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：\n",
      "   - 碗、盘子、锅等烹饪工具散落在桌子上。\n",
      "   - 食材如水果（苹果、橙子）、蔬菜（胡萝卜）等也分布在桌子上。\n",
      "\n",
      "2. **机器人状态**：\n",
      "   - 机器人处于一个特定的位置，可能是准备开始烹饪或已经完成某项任务。\n",
      "   - 状态数据中包含了多个位置坐标，表明\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的厨房环境中的机器人状态。以下是对当前场景的分析和建议的操作步骤：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：桌面上有多个物品，包括碗、盘子、杯子、调料瓶等。\n",
      "2. **机器人状态**：机器人处于一个特定的位置，可能在准备或处理某个任务。\n",
      "\n",
      "### 建议的操作步骤：\n",
      "1. **识别目标**：首先，需要确定机器人要执行的任务。根据当前的物体布局，可能是准备食物或饮料。\n",
      "2. **移动到目标位置**：如果\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，这似乎是一个机器人在执行任务时的状态信息。以下是对当前场景和可能下一步操作的分析：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：桌子上有多个物品，包括碗、杯子、盘子等。\n",
      "2. **动作历史**：机器人似乎正在进行一些与这些物品相关的操作。\n",
      "\n",
      "### 状态数据解读：\n",
      "- **坐标**：状态数据中的坐标信息表明机器人目前的位置和朝向。\n",
      "- **动作指令**：状态数据中的一些数值（如`-8.4133828e-01`）可能\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，机器人似乎处于一个混乱的环境中，桌面上散落着各种物品和工具。以下是对当前场景的分析以及可能的下一步操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：桌面上有多个碗、杯子、刀具等厨房用具。\n",
      "2. **工具位置**：机器人周围有多个工具，包括夹子、铲子、勺子等。\n",
      "3. **机器人状态**：机器人似乎在尝试处理这些物品。\n",
      "\n",
      "### 可能的操作步骤：\n",
      "1. **清理桌面**：\n",
      "   - 使用夹子或铲子将\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，这似乎是一个机器人在处理各种物品的场景。以下是对当前场景的分析和建议的操作：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物品分布**：场景中有多个碗、杯子、水果（如苹果和橙子）、调料瓶等。\n",
      "2. **机器人位置**：机器人位于桌子中央，周围有多个物品需要处理。\n",
      "\n",
      "### 建议操作：\n",
      "1. **识别目标物品**：首先，机器人需要识别出哪些物品是需要处理的目标。例如，如果目标是将所有水果放入一个碗中，那么需要识别出所有的苹果和橙\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含了多个物体和一个机器人。以下是对当前场景的分析：\n",
      "\n",
      "### 场景分析：\n",
      "1. **桌子上的物品**：\n",
      "   - 有一个灰色的碗。\n",
      "   - 有一些杯子（白色和黑色）。\n",
      "   - 一些水果（如苹果、橙子等）。\n",
      "   - 一些调料或调味品容器。\n",
      "\n",
      "2. **机器人位置**：\n",
      "   - 机器人位于桌子的左侧，靠近一个白色的物体（可能是咖啡机或其他设备）。\n",
      "\n",
      "### 下一步操作建议：\n",
      "1. **识别目标**\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含各种物体和可能的动作指令。以下是对当前场景的分析以及一些建议：\n",
      "\n",
      "### 场景分析：\n",
      "1. **物体位置**：场景中包括多个物体，如杯子、碗、盘子、水果等。\n",
      "2. **动作指令**：状态数据中包含了多个动作指令，例如移动到特定位置（如“1.00000000e+00”表示当前位置）。\n",
      "\n",
      "### 建议的操作步骤：\n",
      "1. **识别目标物体**：首先，你需要\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的状态数据来看，机器人似乎处于一个厨房环境中，桌面上有各种物品和工具。以下是对当前场景的分析：\n",
      "\n",
      "1. **物品和工具**：\n",
      "   - 咖啡机（可能是咖啡机）\n",
      "   - 盘子、碗和其他餐具\n",
      "   - 水壶\n",
      "   - 烤面包机\n",
      "   - 面包片\n",
      "\n",
      "2. **机器人状态**：\n",
      "   - 机器人正在执行任务，可能是在准备咖啡或烤面包。\n",
      "\n",
      "根据这些信息，下一步的操作建议如下：\n",
      "\n",
      "### 1. **检查咖啡机的状态**：\n",
      "   - 如果\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "从你提供的机器人状态数据来看，这似乎是一个模拟的机器人环境，其中包含了多个物体和一个机器人。以下是对当前场景的分析：\n",
      "\n",
      "### 场景分析：\n",
      "1. **桌子上的物品**：\n",
      "   - 有一个灰色的碗。\n",
      "   - 有一个白色的杯子。\n",
      "   - 有一些食物（可能是面包片）。\n",
      "   - 有一些调料或酱料容器。\n",
      "   - 有一个红色的物体（可能是玩具或装饰品）。\n",
      "\n",
      "2. **机器人位置**：\n",
      "   - 机器人位于桌子的左侧，靠近灰色的碗。\n",
      "\n",
      "### 下一步操作建议：\n",
      "1. **移动到\n",
      "\u001b[K\u001b[Ftorch.Size([480, 640, 4])\n",
      "torch.Size([66])\n",
      "2025-04-27 10:11:11 [76,495ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.4]) (impl: carb.windowing-glfw.plugin)\n",
      "2025-04-27 10:11:11 [76,654ms] [Warning] [carb] Plugin interface for a client: omni.hydratexture.plugin was already released.\n",
      "2025-04-27 10:11:11 [76,654ms] [Warning] [carb] Plugin interface for a client: omni.hydratexture.plugin was already released.\n",
      "2025-04-27 10:11:11 [76,654ms] [Warning] [carb] Plugin interface for a client: omni.hydratexture.plugin was already released.\n",
      "2025-04-27 10:11:11 [76,654ms] [Warning] [omni.usd] Unexpected reference count of 3 for UsdStage 'anon:0x46950dd0:World1.usd' while being closed in UsdContext (this may indicate it is still resident in memory).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# initialize og env\n",
    "env = BaseEnvironment(configs=\"config/scene_config.yaml\")\n",
    "robot = env.robots[0]\n",
    "# initialize robot controller\n",
    "keyboard_controller = KeyboardRobotController(robot)\n",
    "setup_debug_keys(keyboard_controller, robot, env)\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    count += 1\n",
    "    action = keyboard_controller.get_teleop_action()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # fetch rgb and proprio\n",
    "    rgb_tensor = obs[robot.name][f\"{robot.name}:eyes:Camera:0\"][\"rgb\"]\n",
    "    proprio_tensor = obs[robot.name][\"proprio\"]\n",
    "    # shape of rgb_tensor\n",
    "    print(rgb_tensor.shape)\n",
    "    # shape of proprio_tensor\n",
    "    print(proprio_tensor.shape)\n",
    "    # Remove the alpha channel if present\n",
    "    if rgb_tensor.shape[2] == 4:\n",
    "        rgb_tensor = rgb_tensor[:, :, :3]\n",
    "    # Convert to PIL Image\n",
    "    rgb_img = Image.fromarray(rgb_tensor.numpy())\n",
    "    # Priprio tensor to numpy\n",
    "    proprio_vector = proprio_tensor.numpy()\n",
    "    # Define the input message\n",
    "    result = inference_with_vlm(\n",
    "        rgb_tensor=rgb_tensor,\n",
    "        proprio_tensor=proprio_tensor,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        process_vision_info=process_vision_info,\n",
    "        prompt_template=\"我看到了机器人环境的图像，机器人的状态数据为:{proprio}，请分析这个场景并建议下一步操作。\"\n",
    "    )\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnigibson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
